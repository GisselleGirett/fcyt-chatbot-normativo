"""
benchmark_gpu.py - Script de benchmark para demostraci√≥n
Compara TF-IDF vs B√∫squeda H√≠brida con GPU
"""

import time
import torch
from pathlib import Path
from search_engine import SearchEngine
import matplotlib.pyplot as plt
import numpy as np


def check_system():
    """Verificar configuraci√≥n del sistema"""
    print("="*70)
    print("üîç VERIFICACI√ìN DEL SISTEMA")
    print("="*70)
    
    # GPU
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA Version: {torch.version.cuda}")
        print(f"   VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        print(f"   VRAM Libre: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
    else:
        print("‚ö†Ô∏è  GPU no disponible - usando CPU")
    
    # √çndice
    index_file = Path("indice_tfidf.pkl")
    if index_file.exists():
        size_mb = index_file.stat().st_size / (1024 * 1024)
        print(f"‚úÖ √çndice encontrado: {size_mb:.2f} MB")
    else:
        print("‚ùå √çndice no encontrado. Ejecute: python procesar_pdfs.py")
        return False
    
    print()
    return True


def benchmark_search_methods():
    """Comparar rendimiento de diferentes m√©todos"""
    
    print("="*70)
    print("üèÅ BENCHMARK: TF-IDF vs B√öSQUEDA H√çBRIDA")
    print("="*70)
    
    # Cargar √≠ndice
    print("\nüìö Cargando √≠ndice...")
    engine = SearchEngine.load("indice_tfidf.pkl")
    print(f"‚úÖ Cargado: {len(engine.chunks)} fragmentos\n")
    
    # Consultas de prueba
    queries = [
        "¬øCu√°l es la funci√≥n del docente en PFG?",
        "Requisitos para el proyecto final de grado",
        "¬øC√≥mo se eval√∫an los trabajos acad√©micos?",
        "¬øQu√© dice sobre el tutor del proyecto?",
        "Normativa de investigaci√≥n cient√≠fica"
    ]
    
    results = {
        'queries': [],
        'tfidf_times': [],
        'hybrid_times': [],
        'tfidf_scores': [],
        'hybrid_scores': []
    }
    
    print(f"üìä Ejecutando {len(queries)} consultas...\n")
    print("-"*70)
    
    for i, query in enumerate(queries, 1):
        print(f"\n{i}. Query: \"{query}\"")
        
        # === TF-IDF SOLO (CPU) ===
        start = time.time()
        results_tfidf = engine.search(query, alpha=1.0, top_k=5)
        time_tfidf = (time.time() - start) * 1000
        
        # === H√çBRIDO (GPU) ===
        start = time.time()
        results_hybrid = engine.search(query, alpha=0.7, top_k=5)
        time_hybrid = (time.time() - start) * 1000
        
        # Guardar resultados
        results['queries'].append(f"Q{i}")
        results['tfidf_times'].append(time_tfidf)
        results['hybrid_times'].append(time_hybrid)
        results['tfidf_scores'].append(results_tfidf[0]['score'] if results_tfidf else 0)
        results['hybrid_scores'].append(results_hybrid[0]['score'] if results_hybrid else 0)
        
        # Mostrar resultados
        speedup = time_tfidf / time_hybrid if time_hybrid > 0 else 1
        print(f"   ‚è±Ô∏è  TF-IDF:  {time_tfidf:6.2f}ms | Score: {results_tfidf[0]['score']:.3f}")
        print(f"   ‚ö° H√≠brido: {time_hybrid:6.2f}ms | Score: {results_hybrid[0]['score']:.3f}")
        print(f"   üöÄ Speedup: {speedup:.2f}x")
        
        # Mostrar mejor resultado
        print(f"\n   üìÑ Mejor resultado (H√≠brido):")
        print(f"      {results_hybrid[0]['document']}")
        print(f"      \"{results_hybrid[0]['text'][:150]}...\"")
    
    print("\n" + "="*70)
    
    return results


def generate_plots(results):
    """Generar gr√°ficos de comparaci√≥n"""
    
    print("\nüìä Generando gr√°ficos...")
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Benchmark: TF-IDF vs B√∫squeda H√≠brida con GPU', fontsize=16, fontweight='bold')
    
    x = np.arange(len(results['queries']))
    width = 0.35
    
    # 1. Comparaci√≥n de tiempos
    ax1 = axes[0, 0]
    ax1.bar(x - width/2, results['tfidf_times'], width, label='TF-IDF (CPU)', color='steelblue')
    ax1.bar(x + width/2, results['hybrid_times'], width, label='H√≠brido (GPU)', color='orangered')
    ax1.set_ylabel('Tiempo (ms)')
    ax1.set_title('Tiempo de Respuesta')
    ax1.set_xticks(x)
    ax1.set_xticklabels(results['queries'])
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # 2. Speedup
    ax2 = axes[0, 1]
    speedups = [t1/t2 if t2 > 0 else 1 for t1, t2 in zip(results['tfidf_times'], results['hybrid_times'])]
    colors = ['green' if s > 1 else 'red' for s in speedups]
    ax2.bar(x, speedups, color=colors, alpha=0.7)
    ax2.axhline(y=1, color='black', linestyle='--', linewidth=1)
    ax2.set_ylabel('Speedup Factor')
    ax2.set_title('Speedup (TF-IDF / H√≠brido)')
    ax2.set_xticks(x)
    ax2.set_xticklabels(results['queries'])
    ax2.grid(axis='y', alpha=0.3)
    
    # 3. Comparaci√≥n de scores
    ax3 = axes[1, 0]
    ax3.bar(x - width/2, results['tfidf_scores'], width, label='TF-IDF', color='steelblue')
    ax3.bar(x + width/2, results['hybrid_scores'], width, label='H√≠brido', color='orangered')
    ax3.set_ylabel('Score de Relevancia')
    ax3.set_title('Calidad de Resultados')
    ax3.set_xticks(x)
    ax3.set_xticklabels(results['queries'])
    ax3.legend()
    ax3.grid(axis='y', alpha=0.3)
    
    # 4. Estad√≠sticas globales
    ax4 = axes[1, 1]
    stats = [
        ['Tiempo promedio TF-IDF', f"{np.mean(results['tfidf_times']):.2f} ms"],
        ['Tiempo promedio H√≠brido', f"{np.mean(results['hybrid_times']):.2f} ms"],
        ['Speedup promedio', f"{np.mean(speedups):.2f}x"],
        ['Score promedio TF-IDF', f"{np.mean(results['tfidf_scores']):.3f}"],
        ['Score promedio H√≠brido', f"{np.mean(results['hybrid_scores']):.3f}"],
    ]
    
    ax4.axis('off')
    table = ax4.table(cellText=stats, cellLoc='left', loc='center',
                      colWidths=[0.6, 0.4])
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1, 2)
    ax4.set_title('Estad√≠sticas Globales', pad=20)
    
    plt.tight_layout()
    plt.savefig('benchmark_results.png', dpi=300, bbox_inches='tight')
    print("‚úÖ Gr√°ficos guardados: benchmark_results.png")
    plt.show()


def quality_comparison():
    """Comparar calidad de resultados con ejemplos espec√≠ficos"""
    
    print("\n" + "="*70)
    print("üéØ COMPARACI√ìN DE CALIDAD DE RESULTADOS")
    print("="*70)
    
    engine = SearchEngine.load("indice_tfidf.pkl")
    
    # Consultas donde los embeddings marcan diferencia
    test_cases = [
        {
            'query': "¬øQui√©n supervisa el proyecto?",
            'expected': "docente|tutor|profesor|supervisor"
        },
        {
            'query': "¬øC√≥mo se califica?",
            'expected': "evaluaci√≥n|nota|aprobaci√≥n|calificaci√≥n"
        }
    ]
    
    for i, test in enumerate(test_cases, 1):
        print(f"\n{i}. Query: \"{test['query']}\"")
        print(f"   Palabras clave esperadas: {test['expected']}")
        print("-"*70)
        
        # TF-IDF
        results_tfidf = engine.search(test['query'], alpha=1.0, top_k=3)
        print("\n   üî§ TF-IDF:")
        for j, r in enumerate(results_tfidf[:2], 1):
            print(f"      {j}. Score: {r['score']:.3f}")
            print(f"         \"{r['text'][:120]}...\"")
        
        # H√≠brido
        results_hybrid = engine.search(test['query'], alpha=0.7, top_k=3)
        print("\n   üß† H√≠brido (con embeddings):")
        for j, r in enumerate(results_hybrid[:2], 1):
            print(f"      {j}. Score: {r['score']:.3f}")
            print(f"         \"{r['text'][:120]}...\"")
    
    print("\n" + "="*70)


def main():
    """Funci√≥n principal"""
    
    print("\n" + "="*70)
    print("üéì SISTEMA NORMATIVO FCyT - BENCHMARK DE DEMOSTRACI√ìN")
    print("="*70)
    print()
    
    # Verificar sistema
    if not check_system():
        return
    
    # Benchmark de rendimiento
    results = benchmark_search_methods()
    
    # Comparaci√≥n de calidad
    quality_comparison()
    
    # Generar gr√°ficos
    try:
        generate_plots(results)
    except Exception as e:
        print(f"‚ö†Ô∏è  No se pudieron generar gr√°ficos: {e}")
        print("   (matplotlib no instalado o error de visualizaci√≥n)")
    
    # Resumen final
    print("\n" + "="*70)
    print("‚úÖ BENCHMARK COMPLETADO")
    print("="*70)
    
    avg_tfidf = np.mean(results['tfidf_times'])
    avg_hybrid = np.mean(results['hybrid_times'])
    avg_speedup = avg_tfidf / avg_hybrid
    
    print(f"""
üìä RESUMEN DE RESULTADOS:

   Tiempo promedio TF-IDF:    {avg_tfidf:.2f}ms
   Tiempo promedio H√≠brido:   {avg_hybrid:.2f}ms
   Speedup promedio:          {avg_speedup:.2f}x
   
   Score promedio TF-IDF:     {np.mean(results['tfidf_scores']):.3f}
   Score promedio H√≠brido:    {np.mean(results['hybrid_scores']):.3f}
   
üí° CONCLUSI√ìN:
   La b√∫squeda h√≠brida con embeddings y GPU proporciona:
   - {avg_speedup:.1f}x m√°s r√°pida en promedio
   - Mejor comprensi√≥n sem√°ntica de las consultas
   - Resultados m√°s relevantes para consultas complejas
    """)
    
    print("="*70)
    print()


if __name__ == "__main__":
    main()